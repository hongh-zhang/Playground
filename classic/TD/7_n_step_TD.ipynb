{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c674481f-a378-4091-80f0-470a3d29f9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cdaa92-35bb-4df6-b939-cf2eb055710e",
   "metadata": {},
   "source": [
    "# n-step SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307f44c6-f483-43dc-85d0-69c64bb024d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def returns(rewards, gamma):\n",
    "    \"\"\"Discounted cumulative return on a given reward sequence\"\"\"\n",
    "    return sum([(gamma ** i) * rewards[i] for i in range(len(rewards))])\n",
    "    \n",
    "class N_SARSA():\n",
    "    \n",
    "    def __init__(self, n=3, choice=[0,1,2,3], shape=(SIZE, SIZE, 4), e=0.8, y=0.9, lr=1e-1,):\n",
    "        \n",
    "        self.n  = n\n",
    "        self.states = deque(maxlen=n)\n",
    "        self.actions = deque(maxlen=n)\n",
    "        self.rewards = deque(maxlen=n)\n",
    "        \n",
    "        self.e  = e  # epsilon\n",
    "        self.y  = y  # gamma\n",
    "        self.lr = lr  # learning rate\n",
    "        self.q = np.random.randn(*shape)  # q value array\n",
    "        self.choice = choice  # action space\n",
    "        \n",
    "        self.test_mode = False\n",
    "    \n",
    "    def action(self, state):\n",
    "        \n",
    "        if not self.test_mode and np.random.rand() <= self.e:\n",
    "            action = np.random.choice(self.choice)\n",
    "        else:\n",
    "            action = np.argmax(self.q[state])\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        return action\n",
    "    \n",
    "    def observe(self, reward):\n",
    "        self.rewards.append(reward)\n",
    "    \n",
    "    def learn(self, done):\n",
    "        \n",
    "        if not done:\n",
    "            if len(self.rewards) == self.n:\n",
    "                state, action = self.states[0], self.actions[0]\n",
    "                state_, action_  = self.states[-1], self.actions[-1]\n",
    "                \n",
    "                g = returns(self.rewards, self.gamma) + (self.y ** self.n) * self.q[state_, action_]\n",
    "                self.q[state, action] += self.lr * (g - self.q[state, action])\n",
    "            \n",
    "            # not enough steps have been recorded\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        else:\n",
    "            for i in range(self.n):\n",
    "                state, action, _ = self.cache[i]\n",
    "                if len(self.cache) == self.n:\n",
    "                    g = sum([self.y**i * self.cache[i+j][2] for i in range(self.n-j)])\n",
    "                    self.q[state, action] += self.lr * (g - self.q[state, action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b56e5f-594c-467e-bf5a-a7398c8c5515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def episode3(env, agent):\n",
    "    \n",
    "    state = int2loc(env.reset())\n",
    "    action = agent.action(state)\n",
    "    \n",
    "    for _ in range(MAX_STEPS):\n",
    "        \n",
    "        # take action & observe\n",
    "        state_, reward, done, _ = env.step(action)\n",
    "        agent.observe(reward)\n",
    "        state_ = int2loc(state_)\n",
    "        \n",
    "        # choose next action\n",
    "        action_ = agent.action(state_)\n",
    "        \n",
    "        # update q value\n",
    "        agent.learn(done);\n",
    "        \n",
    "        if done:\n",
    "            return reward\n",
    "        \n",
    "        # iter to next step\n",
    "        state = state_\n",
    "        action = action_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4187805-736d-4a48-ae25-9f297df9baf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 10000\n",
    "MAX_STEPS = 100  # max steps before terminating an episode\n",
    "\n",
    "agent = N_SARSA(n=3, e=1, lr=0.8)\n",
    "returns = []\n",
    "\n",
    "for i in range(EPISODES):\n",
    "    \n",
    "    episode3(env, agent);\n",
    "    \n",
    "    if agent.e >= 0.2:\n",
    "        agent.e *= 0.996\n",
    "        \n",
    "    if i % 20 == 0:\n",
    "        agent.test_mode = True\n",
    "        r = [episode3(env, agent) for _ in range(5)]\n",
    "        returns.append(sum(r) / len(r))\n",
    "        agent.test_mode = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38172d6-5ad2-49c6-8c00-f00cc3609dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(range(len(returns)), returns);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9988d5a-eaab-41df-bb27-889052309f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrows = np.array(['←','↓','→','↑'])\n",
    "np.array([arrows[np.argmax(agent.q, axis=2)[i,j]] \n",
    "          if (env.desc[i,j] == env.desc[0,0]) or (env.desc[i,j] == env.desc[0,1]) else env.desc[i,j]\n",
    "          for i in range(4) for j in range(4)\n",
    "          ]).reshape(4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d501554-cd0c-4ac6-83fa-8a85dd385221",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
